<!DOCTYPE html>
<html lang="fr">
<head>
    {% load static %}
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Journal - MindLense</title>
    <link rel="stylesheet" href="{% static 'css/home.css' %}" />
	<style>
        /* Voice Journal specific styles */
        .voice-hero {
            padding: 40px 0;
            background: linear-gradient(180deg, #ffffff, #f7fbff 55%, #f6fffb);
        }
        
        .voice-hero-content {
            text-align: center;
            max-width: 600px;
            margin: 0 auto;
        }
        
        .voice-hero h1 {
            font-size: 36px;
            line-height: 1.2;
            margin: 0 0 12px;
            color: #101828;
        }
        
        .voice-hero p {
            font-size: 18px;
            color: #475467;
            margin: 0 0 32px;
        }
        
        .recording-section {
            background: #fff;
            border: 1px solid #e5e7eb;
            border-radius: var(--radius-lg);
            padding: 32px;
            margin: 32px 0;
            box-shadow: var(--shadow-sm);
        }
        
        .record-button {
            background: var(--color-secondary);
            color: #fff;
            border: none;
            padding: 14px 28px;
            font-size: 16px;
            font-weight: 700;
            border-radius: var(--radius-md);
            cursor: pointer;
            transition: all 0.2s ease;
            margin: 8px;
            box-shadow: var(--shadow-sm);
        }
        
        .record-button:hover {
            transform: translateY(-1px);
            box-shadow: var(--shadow-md);
        }
        
        .record-button:disabled {
            background: #d1d5db;
            cursor: not-allowed;
            transform: none;
            box-shadow: none;
        }
        
        .stop-button {
            background: var(--color-primary);
        }
        
        .audio-player {
            margin: 24px 0;
            width: 100%;
            border-radius: var(--radius-md);
        }
        
        .status {
            margin: 20px 0;
            padding: 16px;
            border-radius: var(--radius-md);
            text-align: center;
            font-weight: 600;
            border: 1px solid;
        }
        
        .status.recording {
            background: #fef2f2;
            color: #dc2626;
            border-color: #fecaca;
        }
        
        .status.processing {
            background: #eff6ff;
            color: var(--color-primary);
            border-color: #dbeafe;
        }
        
        .status.success {
            background: #f0fdf4;
            color: #16a34a;
            border-color: #bbf7d0;
        }
        
        .status.error {
            background: #fef2f2;
            color: #dc2626;
            border-color: #fecaca;
        }
        
        .results {
            margin-top: 32px;
            padding: 24px;
            background: #fff;
            border: 1px solid #e5e7eb;
            border-radius: var(--radius-lg);
            box-shadow: var(--shadow-sm);
            display: none;
        }
        
        .result-section {
            margin-bottom: 20px;
            padding: 20px;
            background: #f9fafb;
            border-radius: var(--radius-md);
            border-left: 4px solid var(--color-secondary);
        }
        
        .result-section h3 {
            margin: 0 0 12px 0;
            color: #101828;
            font-size: 18px;
            font-weight: 700;
        }
        
        .result-content {
            color: #475467;
            line-height: 1.6;
        }
        
        .sentiment-badge {
            display: inline-block;
            padding: 6px 12px;
            border-radius: 999px;
            font-size: 14px;
            font-weight: 600;
            margin: 4px 8px 4px 0;
        }
        
        .sentiment-positive {
            background: #dcfce7;
            color: #166534;
        }
        
        .sentiment-negative {
            background: #fee2e2;
            color: #991b1b;
        }
        
        .sentiment-neutral {
            background: #f3f4f6;
            color: #374151;
        }
        
        .emotion-badge {
            background: #dbeafe;
            color: #1e40af;
        }
        
        .voice-icon {
            width: 60px;
            height: 60px;
            border-radius: var(--radius-md);
            background: var(--color-secondary);
            display: grid;
            place-items: center;
            font-size: 24px;
            margin: 0 auto 16px;
            color: #fff;
        }
        
        /* Animations pour la respiration */
        @keyframes pulse {
            0% { transform: scale(1); opacity: 0.7; }
            50% { transform: scale(1.1); opacity: 1; }
            100% { transform: scale(1); opacity: 0.7; }
        }
        
        @keyframes breatheIn {
            0% { transform: scale(0.8); }
            100% { transform: scale(1.2); }
        }
        
        @keyframes breatheOut {
            0% { transform: scale(1.2); }
            100% { transform: scale(0.8); }
        }
        
        @keyframes hold {
            0% { transform: scale(1.1); }
            100% { transform: scale(1.1); }
        }
	</style>
</head>
<body>
    {% include 'partials/nav.html' %}

    <main>
        <section class="voice-hero">
            <div class="container voice-hero-content">
                <div class="voice-icon">üé§</div>
                <h1>Voice Journal</h1>
                <p>Enregistrez vos pens√©es et obtenez une analyse instantan√©e de vos √©motions</p>
            </div>
        </section>

        <section class="container">
            <div class="recording-section">
                <button id="recordButton" class="record-button">üé§ Commencer l'enregistrement</button>
                <button id="stopButton" class="record-button stop-button" disabled>‚èπÔ∏è Arr√™ter et analyser</button>
                
                <div id="status" class="status" style="display: none;"></div>
                
                <audio id="audioPlayer" class="audio-player" controls style="display: none;"></audio>
            </div>
            
            <div id="results" class="results">
                <div class="result-section">
                    <h3>üí¨ Message de l'IA</h3>
                    <div id="aiMessage" class="result-content"></div>
                </div>
                <div id="supportCard" class="result-section" style="display:none">
                    <h3>ü´∂ Soutien</h3>
                    <div id="supportText" class="result-content" style="margin-bottom:12px">Tu sembles un peu tendu(e). Veux-tu en parler?</div>
                    <div style="display:flex;gap:8px;align-items:center;flex-wrap:wrap">
                        <button id="talkToAiBtn" class="record-button">ü´∂ Commencer le soutien</button>
                        <button id="stopAiBtn" class="record-button stop-button" disabled>‚èπÔ∏è Terminer l'appel</button>
                        <span id="agentStatus" style="font-weight:600;color:#475467"></span>
                    </div>
                    <div id="agentReply" class="result-content" style="margin-top:10px"></div>
                    <div id="agentPreview" style="margin-top:10px"></div>
                    
                    <!-- Animation de respiration -->
                    <div id="breathingAnimation" style="display:none; margin-bottom:15px; text-align:center; background:#f8fafc; padding:20px; border-radius:12px; border:2px solid #e2e8f0;">
                        <div id="breathingStep" style="font-size:14px; color:#64748b; margin-bottom:8px; font-weight:500;">
                            <span id="stepText">√âtape 1/3</span>
                        </div>
                        <div id="breathingTimer" style="font-size:32px; font-weight:bold; color:#4A90E2; margin-bottom:12px;">
                            <span id="timerText">Pr√™t</span>
                        </div>
                        <div id="breathingProgress" style="width:100%; height:12px; background:#e2e8f0; border-radius:6px; overflow:hidden; margin-bottom:12px;">
                            <div id="progressBar" style="width:0%; height:100%; background:linear-gradient(90deg, #4A90E2, #50E3C2); transition:width 0.1s linear;"></div>
                        </div>
                        <div id="breathingInstruction" style="font-size:18px; color:#334155; margin-bottom:12px; font-weight:600;">
                            <span id="instructionText">En attente...</span>
                        </div>
                        <div id="breathingVisual" style="width:80px; height:80px; margin:0 auto; border-radius:50%; background:#4A90E2; opacity:0.8; transition:all 0.3s ease; box-shadow:0 4px 12px rgba(74, 144, 226, 0.3);"></div>
                    </div>
                </div>
                <div class="result-section">
                    <h3>üìù Transcription</h3>
                    <div id="transcription" class="result-content"></div>
		</div>

                <div class="result-section">
                    <h3>üòä Sentiment du texte</h3>
                    <div id="textSentiment" class="result-content"></div>
                </div>
                
                <div class="result-section">
                    <h3>üéµ √âmotion audio</h3>
                    <div id="audioEmotion" class="result-content"></div>
                </div>

                <div class="result-section">
                    <h3>üé∂ Recommandations musicales</h3>
                    <div id="musicList" class="result-content" style="display:grid;gap:12px"></div>
		</div>
	</div>
        </section>
    </main>

    {% include 'partials/footer.html' %}

	<script>
        class VoiceJournal {
            constructor() {
                this.mediaRecorder = null;
                this.audioChunks = [];
                this.isRecording = false;
                this.chunkCount = 0;
                
                this.recordButton = document.getElementById('recordButton');
                this.stopButton = document.getElementById('stopButton');
                this.status = document.getElementById('status');
                this.audioPlayer = document.getElementById('audioPlayer');
                this.results = document.getElementById('results');
                
                this.setupEventListeners();
                console.log('[VoiceJournal] Initialized');
            }
            
            setupEventListeners() {
                this.recordButton.addEventListener('click', () => this.startRecording());
                this.stopButton.addEventListener('click', () => this.stopRecording());
                console.log('[VoiceJournal] Event listeners attached');
            }
            
            async startRecording() {
                try {
                    console.log('[VoiceJournal] Requesting microphone permission...');
				const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                    console.log('[VoiceJournal] Microphone stream obtained');
                    
                    this.mediaRecorder = new MediaRecorder(stream);
                    this.audioChunks = [];
                    this.chunkCount = 0;
                    
                    this.mediaRecorder.ondataavailable = (event) => {
                        this.audioChunks.push(event.data);
                        this.chunkCount += 1;
                        console.log(`[VoiceJournal] ondataavailable: chunk ${this.chunkCount}, size=${event.data?.size}`);
                    };
                    
                    this.mediaRecorder.onstop = () => {
                        console.log(`[VoiceJournal] Recorder stopped. Total chunks: ${this.chunkCount}`);
                        this.processRecording();
                    };
                    
                    this.mediaRecorder.start();
                    console.log('[VoiceJournal] Recording started');
                    this.isRecording = true;
                    
                    this.updateUI('recording', 'üî¥ Enregistrement en cours... Cliquez sur "Arr√™ter et analyser" quand vous avez termin√©');
                    this.recordButton.disabled = true;
                    this.stopButton.disabled = false;
                    
                } catch (error) {
                    console.error('Error accessing microphone:', error);
                    this.updateUI('error', '‚ùå Erreur d\'acc√®s au microphone. Veuillez v√©rifier les permissions.');
                }
            }
            
            stopRecording() {
                if (this.mediaRecorder && this.isRecording) {
                    console.log('[VoiceJournal] Stopping recording...');
                    this.mediaRecorder.stop();
                    this.isRecording = false;
                    
                    this.updateUI('processing', '‚è≥ Traitement de votre audio...');
                    this.stopButton.disabled = true;
                }
            }
            
            async processRecording() {
                try {
                    console.log(`[VoiceJournal] Processing recording. Chunks: ${this.audioChunks.length}`);
                    const audioBlob = new Blob(this.audioChunks, { type: 'audio/webm' });
                    console.log(`[VoiceJournal] Blob created. Size=${audioBlob.size} type=${audioBlob.type}`);
                    
                    // Show the recorded audio
                    const audioUrl = URL.createObjectURL(audioBlob);
                    this.audioPlayer.src = audioUrl;
                    this.audioPlayer.style.display = 'block';
                    console.log('[VoiceJournal] Audio preview ready');
                    
                    // Send to server for processing
                    const formData = new FormData();
                    formData.append('audio', audioBlob, 'recording.webm');
                    console.log('[VoiceJournal] Sending fetch -> /voice-journal/process-audio/');
                    const t0 = performance.now();
                    
                    const response = await fetch('/voice-journal/process-audio/', {
                        method: 'POST',
                        body: formData,
                        headers: {
                            'X-CSRFToken': this.getCSRFToken()
                        }
                    });
                    const t1 = performance.now();
                    console.log(`[VoiceJournal] Fetch completed. status=${response.status} (${(t1 - t0).toFixed(0)} ms)`);
                    let result = null;
                    try {
                        result = await response.json();
                    } catch (e) {
                        console.warn('[VoiceJournal] Failed to parse JSON response', e);
                    }
                    console.log('[VoiceJournal] Server response:', result);
                    if (result && typeof result.transcription === 'string') {
                        console.log('[VoiceJournal] Transcription (main):', result.transcription);
                    }
                    
                    if (result.success) {
                        this.displayResults(result);
                        this.updateUI('success', '‚úÖ Analyse termin√©e avec succ√®s !');
                    } else {
                        const missing = Array.isArray(result?.missing) ? `\nManquant: ${result.missing.join(', ')}` : '';
                        this.updateUI('error', `‚ùå Erreur: ${result?.error || 'Erreur inconnue'}` + missing);
                        console.error('[VoiceJournal] Error result:', result);
                    }
                    
                } catch (error) {
                    console.error('Error processing audio:', error);
                    this.updateUI('error', '‚ùå Erreur lors du traitement audio. Veuillez r√©essayer.');
                } finally {
                    this.recordButton.disabled = false;
                    this.stopButton.disabled = true;
                }
            }
            
            displayResults(result) {
                document.getElementById('transcription').textContent = result.transcription;
                
                const sentimentClass = result.text_sentiment.toLowerCase();
                document.getElementById('textSentiment').innerHTML = `
                    <span class="sentiment-badge sentiment-${sentimentClass}">
                        ${result.text_sentiment} (${result.text_sentiment_score.toFixed(2)})
                    </span>
                `;
                
                document.getElementById('audioEmotion').innerHTML = `
                    <span class="emotion-badge">
                        ${result.audio_emotion} (${result.audio_emotion_score.toFixed(2)})
                    </span>
                `;

                // AI message
                const aiMsgEl = document.getElementById('aiMessage');
                aiMsgEl.textContent = result.ai_message || '';

                // Suggestion card for sad/stressed
                const supportCard = document.getElementById('supportCard');
                const supportText = document.getElementById('supportText');
                const mood = (result.emotion || '').toLowerCase();
                if (mood === 'sad' || mood === 'stressed') {
                    supportText.textContent = mood === 'sad'
                        ? "Tu sembles un peu triste. Veux-tu en parler √† voix haute? Clique sur 'Commencer le soutien' pour d√©marrer une conversation avec l'assistant."
                        : "Tu sembles stress√©(e). Souhaites-tu une courte discussion guid√©e? Clique sur 'Commencer le soutien' pour d√©marrer une conversation avec l'assistant.";
                    supportCard.style.display = 'block';
                    supportCard.dataset.mood = mood;
                    // Ne pas d√©marrer automatiquement - attendre que l'utilisateur clique sur le bouton
                } else {
                    supportCard.style.display = 'none';
                }

                // Dynamic theme (apply to hero background)
                if (result.theme && result.theme.primary && result.theme.secondary) {
                    const hero = document.querySelector('.voice-hero');
                    hero.style.background = `linear-gradient(180deg, ${result.theme.primary}, ${result.theme.secondary})`;
                }

                // Music cards
                const musicList = document.getElementById('musicList');
                musicList.innerHTML = '';
                (result.music || []).forEach(item => {
                    const card = document.createElement('div');
                    card.style.display = 'grid';
                    card.style.gridTemplateColumns = '160px 1fr';
                    card.style.gap = '12px';
                    card.style.alignItems = 'center';
                    card.style.padding = '10px';
                    card.style.border = '1px solid #e5e7eb';
                    card.style.borderRadius = '12px';

                    if (item.embedUrl) {
                        const iframe = document.createElement('iframe');
                        iframe.width = '160';
                        iframe.height = '90';
                        iframe.src = item.embedUrl;
                        iframe.title = item.title || 'YouTube player';
                        iframe.frameBorder = '0';
                        iframe.referrerPolicy = 'strict-origin-when-cross-origin';
                        iframe.allow = 'accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share';
                        iframe.allowFullscreen = true;
                        card.appendChild(iframe);
                    } else {
                        const ph = document.createElement('div');
                        ph.style.width = '160px';
                        ph.style.height = '90px';
                        ph.style.borderRadius = '8px';
                        ph.style.background = '#f3f4f6';
                        ph.style.display = 'grid';
                        ph.style.placeItems = 'center';
                        ph.textContent = 'Preview indisponible';
                        card.appendChild(ph);
                    }

                    const meta = document.createElement('div');
                    meta.style.fontWeight = '700';
                    meta.style.fontSize = '14px';
                    meta.textContent = item.title || 'Recommended music';
                    card.appendChild(meta);

                    musicList.appendChild(card);
                });
                
                this.results.style.display = 'block';
                
                // Scroll to results
                this.results.scrollIntoView({ behavior: 'smooth' });
            }
            
            updateUI(type, message) {
                this.status.textContent = message;
                this.status.className = `status ${type}`;
                this.status.style.display = 'block';
            }
            
            getCSRFToken() {
                return document.querySelector('[name=csrfmiddlewaretoken]')?.value || 
                       document.cookie.match(/csrftoken=([^;]+)/)?.[1] || '';
            }
        }
        
        // Initialize the voice journal when the page loads
        document.addEventListener('DOMContentLoaded', () => {
            new VoiceJournal();
            setupVoiceAgent();
		});

        // --- Voice Agent JS ---
        function setupVoiceAgent() {
            const talkBtn = document.getElementById('talkToAiBtn');
            const stopBtn = document.getElementById('stopAiBtn');
            const statusEl = document.getElementById('agentStatus');
            const replyEl = document.getElementById('agentReply');
            const supportCard = document.getElementById('supportCard');
            let agentRecorder = null;
            let agentChunks = [];
            let agentStream = null;
            let audioCtx = null;
            let analyser = null;
            let rafId = null;
            let inCall = false;
            let listeningActive = false;

            window.voiceAgentAutoStart = async function() {
                if (inCall) return;
                await startConversation();
            }

            async function startConversation() {
                inCall = true;
                replyEl.textContent = '';
                statusEl.textContent = 'Connexion‚Ä¶';
                talkBtn.disabled = true;
                stopBtn.disabled = false;
                try {
                    const fd = new FormData();
                    const mood = supportCard?.dataset?.mood || '';
                    if (mood) fd.append('mood', mood);
                    fd.append('start', '1');
                    const res = await fetch('/voice-journal/api/voice-agent-turn/', {
                        method: 'POST',
                        body: fd,
                        headers: { 'X-CSRFToken': (document.querySelector('[name=csrfmiddlewaretoken]')?.value || '') }
                    });
                    const data = await res.json();
                    if (data && typeof data.user_text === 'string') {
                        console.log('[VoiceAgent] Transcription (turn):', data.user_text);
                    }
                    if (!data.success) throw new Error(data.error || 'start failed');
                    replyEl.textContent = data.reply_text || '';
                    statusEl.textContent = 'Assistant parle‚Ä¶';
                    speakText(replyEl.textContent, (data.mood || ''), () => beginListening());
                } catch (e) {
                    statusEl.textContent = 'Erreur d√©marrage';
                    inCall = false;
                    talkBtn.disabled = false;
                    stopBtn.disabled = true;
                }
            }

            async function beginListening() {
                if (!inCall) return;
                if (listeningActive) return;
                statusEl.textContent = '√âcoute‚Ä¶ (parlez, prenez votre temps)';
                agentChunks = [];
                listeningActive = true;
                try {
                    agentStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                    audioCtx = new (window.AudioContext || window.webkitAudioContext)();
                    const source = audioCtx.createMediaStreamSource(agentStream);
                    analyser = audioCtx.createAnalyser();
                    analyser.fftSize = 2048;
                    source.connect(analyser);
                    const dataArray = new Uint8Array(analyser.fftSize);
                    let mime = 'audio/webm;codecs=opus';
                    if (!MediaRecorder.isTypeSupported(mime)) mime = 'audio/webm';
                    if (!MediaRecorder.isTypeSupported(mime)) mime = '';
                    agentRecorder = new MediaRecorder(agentStream, mime ? { mimeType: mime } : undefined);
                    agentRecorder.ondataavailable = (e) => { if (e && e.data && e.data.size) agentChunks.push(e.data); };
                    // Use a timeslice so dataavailable fires periodically even for short utterances
                    agentRecorder.start(300);

                    let silentMs = 0;
                    const threshold = 8; // Seuil plus sensible pour d√©tecter le silence
                    const minSpeechMs = 800; // Plus de temps minimum avant de pouvoir d√©tecter le silence
                    let totalMs = 0;
                    const intervalMs = 100;
                    const tick = async () => {
                        if (!inCall) return;
                        analyser.getByteTimeDomainData(dataArray);
                        let sum = 0;
                        for (let i = 0; i < dataArray.length; i++) {
                            const v = dataArray[i] - 128;
                            sum += Math.abs(v);
                        }
                        const avg = sum / dataArray.length;
                        totalMs += intervalMs;
                        if (avg < threshold && totalMs > minSpeechMs) {
                            silentMs += intervalMs;
                        } else {
                            silentMs = 0;
                        }
                        
                        // D√©tecter si on attend une r√©ponse musicale simple (oui/non)
                        const currentReply = replyEl.textContent.toLowerCase();
                        const isMusicResponse = currentReply.includes('aimes') || 
                                               currentReply.includes('extrait') || 
                                               currentReply.includes('musique') ||
                                               currentReply.includes('oui/non') ||
                                               currentReply.includes('ou pas');
                        
                        // R√©duire le d√©lai de silence pour r√©pondre plus vite
                        const isQuickResponse = isMusicResponse || 
                                               currentReply.includes('dis-moi') ||
                                               currentReply.includes('veux-tu') ||
                                               currentReply.includes('souhaites-tu') ||
                                               currentReply.includes('besoin d\'autre chose');
                        
                        const silenceThreshold = isQuickResponse ? 800 : 1800; // 0.8s pour r√©ponses rapides, 1.8s pour le reste
                        
                        if (silentMs >= silenceThreshold) {
                            // D√©lai suppl√©mentaire pour s'assurer que l'utilisateur a termin√©
                            statusEl.textContent = 'Traitement‚Ä¶ (attendez)';
                            
                            const delay = isQuickResponse ? 100 : 300; // D√©lai tr√®s r√©duit pour les r√©ponses rapides
                            setTimeout(async () => {
                                await finalizeTurn();
                            }, delay);
                            return;
                        }
                        rafId = setTimeout(tick, intervalMs);
                    };
                    rafId = setTimeout(tick, intervalMs);
                } catch (e) {
                    statusEl.textContent = 'Erreur micro';
                    endCallCleanup();
                }
            }

            async function finalizeTurn() {
                try { if (rafId) { clearTimeout(rafId); rafId = null; } } catch {}
                // Stop the recorder and WAIT for its onstop to flush last chunk
                try {
                    if (agentRecorder && agentRecorder.state !== 'inactive') {
                        const stopped = new Promise(resolve => {
                            const prevOnStop = agentRecorder.onstop;
                            agentRecorder.onstop = (...args) => { try { prevOnStop && prevOnStop(...args); } catch {} resolve(); };
                        });
                        agentRecorder.stop();
                        await stopped;
                    }
                } catch {}
                try { if (agentStream) agentStream.getTracks().forEach(t => t.stop()); } catch {}
                try { if (audioCtx && audioCtx.state !== 'closed') audioCtx.close(); } catch {}
                listeningActive = false;
                statusEl.textContent = 'Analyse‚Ä¶';

                try {
                    const size = agentChunks.reduce((s, b) => s + (b?.size || 0), 0);
                    console.log('[VoiceAgent] Turn size (bytes):', size);
                    if (!size) {
                        // Pendant les exercices de respiration, ne pas consid√©rer le silence comme une erreur
                        const supportCard = document.getElementById('supportCard');
                        const currentReply = replyEl.textContent.toLowerCase();
                        const isBreathingExercise = currentReply.includes('respir') || 
                                                   currentReply.includes('inspire') || 
                                                   currentReply.includes('bloque') || 
                                                   currentReply.includes('expire');
                        
                        if (isBreathingExercise) {
                            statusEl.textContent = "Continuez l'exercice...";
                            // Envoyer une requ√™te vide pour continuer l'exercice
                            const fd = new FormData();
                            fd.append('audio', new Blob(), 'empty.webm');
                            const mood = supportCard?.dataset?.mood || '';
                            if (mood) fd.append('mood', mood);
                            
                            try {
                                const res = await fetch('/voice-journal/api/voice-agent-turn/', {
                                    method: 'POST',
                                    body: fd,
                                    headers: { 'X-CSRFToken': (document.querySelector('[name=csrfmiddlewaretoken]')?.value || '') }
                                });
                                const data = await res.json();
                                if (data.success) {
                                    replyEl.textContent = data.reply_text || '';
                                    statusEl.textContent = 'Assistant parle‚Ä¶';
                                    speakText(replyEl.textContent, (data.mood || ''), () => {
                                        if (data.end_call) {
                                            stopCall();
                                        } else {
                                            beginListening();
                                        }
                                    });
                                    return;
                                }
                            } catch (e) {
                                console.log('Erreur lors de la continuation de l\'exercice:', e);
                            }
                        }
                        
                        statusEl.textContent = "Je n'ai rien capt√©. R√©essayons.";
                        if (inCall) setTimeout(() => beginListening(), 400);
                        return;
                    }
                    const blob = new Blob(agentChunks, { type: 'audio/webm' });
                    const fd = new FormData();
                    fd.append('audio', blob, 'agent_turn.webm');
                    const mood = supportCard?.dataset?.mood || '';
                    if (mood) fd.append('mood', mood);
                    const res = await fetch('/voice-journal/api/voice-agent-turn/', {
                        method: 'POST',
                        body: fd,
                        headers: { 'X-CSRFToken': (document.querySelector('[name=csrfmiddlewaretoken]')?.value || '') }
                    });
                    const data = await res.json();
                    if (!data.success) throw new Error(data.error || 'turn failed');
                    if (typeof data.intent === 'string') {
                        console.log('[VoiceAgent] Detected intent:', data.intent);
                    }
                    replyEl.textContent = data.reply_text || '';
                    // Render optional music preview and manage flow
                    const pv = document.getElementById('agentPreview');
                    const hadPreview = !!pv.dataset.currentEmbed;
                    if (data.preview && data.preview.embedUrl) {
                        // Keep preview visible and reusable; only replace if new
                        if (pv.dataset.currentEmbed !== data.preview.embedUrl) {
                            pv.innerHTML = '';
                            const iframe = document.createElement('iframe');
                            iframe.width = '280';
                            iframe.height = '158';
                            iframe.src = data.preview.embedUrl;
                            iframe.frameBorder = '0';
                            iframe.allow = 'autoplay; encrypted-media; picture-in-picture';
                            iframe.allowFullscreen = true;
                            pv.appendChild(iframe);
                            // Add a small clickable link below
                            const link = document.createElement('a');
                            link.href = data.preview.embedUrl.replace('youtube-nocookie.com/embed/', 'youtube.com/watch?v=');
                            link.target = '_blank';
                            link.rel = 'noopener noreferrer';
                            link.textContent = 'Ouvrir sur YouTube';
                            link.style.display = 'block';
                            link.style.marginTop = '6px';
                            pv.appendChild(link);
                            pv.dataset.currentEmbed = data.preview.embedUrl;
                        }
                        // During preview, wait for video to finish, then ask for feedback
                        statusEl.textContent = "Extrait en cours‚Ä¶";
                        if (data.end_call) {
                            stopCall();
                        } else {
                            // Attendre que l'extrait se termine, puis demander l'avis rapidement
                            setTimeout(() => { 
                                if (inCall) {
                                    statusEl.textContent = 'Assistant parle‚Ä¶';
                                    speakText("Tu aimes ou pas ?", '', () => {
                                        if (inCall) beginListening();
                                    });
                                }
                            }, 8000); // 8 secondes pour l'extrait, puis question imm√©diate
                        }
                    } else {
                        // No preview; proceed with TTS + normal loop
                        // Do not clear pv here so the user can still click the previous video
                        statusEl.textContent = 'Assistant parle‚Ä¶';
                        speakText(replyEl.textContent, (data.mood || ''), () => {
                            if (data.end_call) {
                                stopCall();
                            } else {
                                beginListening();
                            }
                        });
                    }
                } catch (e) {
                    statusEl.textContent = 'Erreur agent';
                    // keep call but allow retry listening
                    if (inCall) setTimeout(() => beginListening(), 600);
                }
            }

            function speakText(text, mood, onend) {
                try {
                    const utter = new SpeechSynthesisUtterance(text);
                    utter.lang = 'fr-FR';
                    utter.rate = 1.0;
                    if (mood === 'sad') utter.rate = 0.95;
                    if (mood === 'stressed') utter.rate = 0.98;
                    window.speechSynthesis.cancel();
                    
                    // D√©tecter les instructions de respiration AVANT de parler
                    let breathingAnimationType = null;
                    let breathingDuration = 0;
                    if (text && typeof text === 'string') {
                        const breathingText = text.toLowerCase();
                        
                        // Masquer l'animation si l'exercice est termin√©
                        if (breathingText.includes('tr√®s bien') || 
                            breathingText.includes('refait') || 
                            breathingText.includes('besoin d\'autre chose') ||
                            breathingText.includes('souhaites-tu refaire')) {
                            setTimeout(() => {
                                const animationEl = document.getElementById('breathingAnimation');
                                animationEl.style.display = 'none';
                                breathingStepCount = 0;
                            }, 1000);
                        }
                        // D√©tecter "Inspire 4 secondes"
                        else if (breathingText.includes('inspire 4')) {
                            breathingAnimationType = 'inspire';
                            breathingDuration = 4;
                        }
                        // D√©tecter "Bloque 4 secondes"
                        else if (breathingText.includes('bloque 4')) {
                            breathingAnimationType = 'hold';
                            breathingDuration = 4;
                        }
                        // D√©tecter "Expire 6 secondes"
                        else if (breathingText.includes('expire') && breathingText.includes('6')) {
                            breathingAnimationType = 'expire';
                            breathingDuration = 6;
                        }
                    }
                    
                    // Cr√©er le callback qui g√®re √† la fois onend et l'animation
                    utter.onend = () => { 
                        if (typeof onend === 'function') onend();
                        // D√©clencher l'animation apr√®s que l'AI termine de parler
                        if (breathingAnimationType) {
                            triggerBreathingStep(breathingAnimationType, breathingDuration);
                        }
                    };
                    
                    window.speechSynthesis.speak(utter);
                } catch {}
            }
            
            // Animation de respiration avec gestion des √©tapes
            let breathingStepCount = 0;
            let breathingInterval = null;
            
            function triggerBreathingStep(type, duration) {
                const animationEl = document.getElementById('breathingAnimation');
                const stepEl = document.getElementById('stepText');
                const timerEl = document.getElementById('timerText');
                const progressEl = document.getElementById('progressBar');
                const instructionEl = document.getElementById('instructionText');
                const visualEl = document.getElementById('breathingVisual');
                
                // Afficher l'animation si ce n'est pas d√©j√† fait
                animationEl.style.display = 'block';
                
                // Arr√™ter toute animation en cours
                if (breathingInterval) {
                    clearInterval(breathingInterval);
                    breathingInterval = null;
                }
                
                // D√©finir le texte et le style selon le type
                let text, style, stepNum;
                if (type === 'inspire') {
                    text = 'Inspirez pendant 4 secondes';
                    style = 'inspire';
                    stepNum = 1;
                    breathingStepCount = 1;  // Premi√®re vraie √©tape
                } else if (type === 'hold') {
                    text = 'Bloquez pendant 4 secondes';
                    style = 'hold';
                    stepNum = 2;
                    breathingStepCount = 2;  // Deuxi√®me √©tape
                } else if (type === 'expire') {
                    text = 'Expirez pendant 6 secondes';
                    style = 'expire';
                    stepNum = 3;
                    breathingStepCount = 3;  // Troisi√®me √©tape
                }
                
                // Mettre √† jour l'interface
                stepEl.textContent = `√âtape ${stepNum}/3`;
                instructionEl.textContent = text;
                timerEl.textContent = duration;
                progressEl.style.width = '0%';
                
                // Animation visuelle selon le type
                updateVisualStyle(visualEl, style);
                
                let timeLeft = duration;
                
                breathingInterval = setInterval(() => {
                    timeLeft--;
                    timerEl.textContent = timeLeft;
                    
                    // Mettre √† jour la barre de progression
                    const progress = ((duration - timeLeft) / duration) * 100;
                    progressEl.style.width = progress + '%';
                    
                    // Animation du cercle
                    const scale = 0.8 + (progress / 100) * 0.4;
                    visualEl.style.transform = `scale(${scale})`;
                    
                    if (timeLeft <= 0) {
                        clearInterval(breathingInterval);
                        timerEl.textContent = '‚úì';
                        progressEl.style.width = '100%';
                        visualEl.style.transform = 'scale(1)';
                        breathingInterval = null;
                        
                        // Si ce n'est pas la derni√®re √©tape, r√©cup√©rer automatiquement l'√©tape suivante
                        if (breathingStepCount < 3 && inCall) {
                            // Petit d√©lai pour que l'utilisateur voie le ‚úì
                            setTimeout(() => {
                                requestNextBreathingStep();
                            }, 300);
                        }
                    }
                }, 1000);
            }
            
            async function requestNextBreathingStep() {
                // Envoyer une requ√™te vide au serveur pour obtenir l'√©tape suivante
                try {
                    const fd = new FormData();
                    fd.append('audio', new Blob(), 'empty.webm');
                    const supportCard = document.getElementById('supportCard');
                    const mood = supportCard?.dataset?.mood || '';
                    if (mood) fd.append('mood', mood);
                    
                    const res = await fetch('/voice-journal/api/voice-agent-turn/', {
                        method: 'POST',
                        body: fd,
                        headers: { 'X-CSRFToken': (document.querySelector('[name=csrfmiddlewaretoken]')?.value || '') }
                    });
                    const data = await res.json();
                    if (data.success) {
                        const replyEl = document.getElementById('agentReply');
                        replyEl.textContent = data.reply_text || '';
                        statusEl.textContent = 'Assistant parle‚Ä¶';
                        
                        const replyText = replyEl.textContent.toLowerCase();
                        
                        // Si le texte contient "Tr√®s bien" ou "refait", on a termin√© l'exercice
                        if (replyText.includes('tr√®s bien') || replyText.includes('refait') || replyText.includes('besoin d\'autre chose')) {
                            // Attendre que l'utilisateur r√©ponde avant de commencer √† √©couter
                            speakText(replyEl.textContent, (data.mood || ''), () => {
                                if (data.end_call) {
                                    stopCall();
                                } else {
                                    // Laisser un petit d√©lai puis commencer √† √©couter
                                    setTimeout(() => {
                                        if (inCall) beginListening();
                                    }, 500);
                                }
                            });
                        } 
                        // Si c'est l'introduction "commence une respiration", obtenir la prochaine √©tape imm√©diatement
                        else if (replyText.includes('commence une respiration')) {
                            speakText(replyEl.textContent, (data.mood || ''), () => {
                                // Attendre un instant puis obtenir l'√©tape suivante (Inspire 4 secondes)
                                setTimeout(() => {
                                    requestNextBreathingStep();
                                }, 300);
                            });
                        }
                        // C'est une instruction de respiration, d√©marrer l'animation apr√®s que l'AI parle
                        else {
                            speakText(replyEl.textContent, (data.mood || ''), () => {
                                if (data.end_call) {
                                    stopCall();
                                }
                                // Ne pas appeler beginListening - l'animation sera d√©clench√©e automatiquement
                            });
                        }
                    }
                } catch (e) {
                    console.log('Erreur lors de la r√©cup√©ration de l\'√©tape suivante:', e);
                }
            }
            
            function updateVisualStyle(element, style) {
                element.className = '';
                switch(style) {
                    case 'inspire':
                        element.style.background = 'linear-gradient(45deg, #4A90E2, #50E3C2)';
                        element.style.animation = 'pulse 1s infinite';
                        break;
                    case 'hold':
                        element.style.background = '#F5A623';
                        element.style.animation = 'none';
                        break;
                    case 'expire':
                        element.style.background = 'linear-gradient(45deg, #50E3C2, #4A90E2)';
                        element.style.animation = 'pulse 1.5s infinite';
                        break;
                    default:
                        element.style.background = '#4A90E2';
                        element.style.animation = 'none';
                }
            }

            function endCallCleanup() {
                try { window.speechSynthesis.cancel(); } catch {}
                try { if (rafId) { clearTimeout(rafId); rafId = null; } } catch {}
                try { if (agentRecorder && agentRecorder.state !== 'inactive') agentRecorder.stop(); } catch {}
                try { if (agentStream) agentStream.getTracks().forEach(t => t.stop()); } catch {}
                try { if (audioCtx && audioCtx.state !== 'closed') audioCtx.close(); } catch {}
                try { if (breathingInterval) { clearInterval(breathingInterval); breathingInterval = null; } } catch {}
                try { document.getElementById('breathingAnimation').style.display = 'none'; } catch {}
                breathingStepCount = 0; // Reset breathing step counter
                talkBtn.disabled = false;
                stopBtn.disabled = true;
                statusEl.textContent = '';
            }

            async function stopCall() {
                inCall = false;
                endCallCleanup();
            }

            if (talkBtn) talkBtn.addEventListener('click', startConversation);
            if (stopBtn) stopBtn.addEventListener('click', stopCall);
        }
	</script>
</body>
</html>
